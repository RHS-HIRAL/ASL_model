{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e7RQCs9QhgXe"
   },
   "outputs": [],
   "source": [
    "# 1ï¸âƒ£ Mount Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jd3uRIn3h2Gw"
   },
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ Paths (adjust to your folder structure)\n",
    "DRIVE_BASE = '/asl_model_train/' #content/drive/MyDrive/\n",
    "TRAIN_H5 = os.path.join(DRIVE_BASE, 'train_data.h5')\n",
    "VAL_H5   = os.path.join(DRIVE_BASE, 'val_data.h5')\n",
    "CHECKPOINT_DIR = os.path.join(DRIVE_BASE, 'checkpoints')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BEST_MODEL_PATH   = os.path.join(CHECKPOINT_DIR, 'model_best.h5')\n",
    "LATEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'model_latest.h5')\n",
    "EPOCH_FILE        = os.path.join(CHECKPOINT_DIR, 'epoch.txt')\n",
    "LOG_CSV           = os.path.join(CHECKPOINT_DIR, 'training_log.csv')\n",
    "\n",
    "# 3ï¸âƒ£ Hyperâ€‘parameters\n",
    "SEQ_LEN     = 500      # match how you preprocessed\n",
    "FEATURE_DIM = 126\n",
    "NUM_CLASSES = 1000\n",
    "BATCH_SIZE  = 25\n",
    "EPOCHS      = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BCIjfWhqh_-R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ CPU/GPU strategy\n"
     ]
    }
   ],
   "source": [
    "# 4ï¸âƒ£ (Optional) TPU strategy\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"âš¡ TPU enabled\")\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"âš™ï¸ CPU/GPU strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oWPZrb5PiCmJ"
   },
   "outputs": [],
   "source": [
    "# 5ï¸âƒ£ Data generator\n",
    "def data_generator(h5_path, batch_size):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        X, y = f['X'], f['y']\n",
    "        size = len(X)\n",
    "        while True:\n",
    "            idxs = np.arange(size)\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, size, batch_size):\n",
    "                batch = idxs[start:start+batch_size]\n",
    "                yield X[batch], y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0lmyoKgPjQNa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting new training run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hiral\\Projects\\Sign Language Projects\\model_train\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 6ï¸âƒ£ Model builder (inside strategy scope)\n",
    "with strategy.scope():\n",
    "    def build_model(seq_len, feature_dim, num_classes):\n",
    "        m = Sequential([\n",
    "            Masking(mask_value=0., input_shape=(seq_len, feature_dim)),\n",
    "            LSTM(64, return_sequences=False),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        m.compile(optimizer=Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        return m\n",
    "\n",
    "    # 7ï¸âƒ£ Resume logic\n",
    "    if os.path.exists(LATEST_MODEL_PATH):\n",
    "        print(\"ðŸ”„ Resuming from last checkpointâ€¦\")\n",
    "        model = tf.keras.models.load_model(LATEST_MODEL_PATH)\n",
    "        # read last epoch (stored as completed epochs)\n",
    "        with open(EPOCH_FILE, 'r') as f:\n",
    "            initial_epoch = int(f.read().strip())\n",
    "    else:\n",
    "        print(\"ðŸš€ Starting new training run\")\n",
    "        model = build_model(SEQ_LEN, FEATURE_DIM, NUM_CLASSES)\n",
    "        initial_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ðŸš« Required training or validation H5 file not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 8ï¸âƒ£ Compute steps\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(TRAIN_H5) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(VAL_H5):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš« Required training or validation H5 file not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(TRAIN_H5, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ðŸš« Required training or validation H5 file not found."
     ]
    }
   ],
   "source": [
    "# 8ï¸âƒ£ Compute steps\n",
    "if not os.path.exists(TRAIN_H5) or not os.path.exists(VAL_H5):\n",
    "    raise FileNotFoundError(\"ðŸš« Required training or validation H5 file not found.\")\n",
    "\n",
    "with h5py.File(TRAIN_H5, 'r') as f:\n",
    "    train_size = len(f['X'])\n",
    "with h5py.File(VAL_H5, 'r') as f:\n",
    "    val_size = len(f['X'])\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "val_steps = val_size // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9ï¸âƒ£ Callbacks\n",
    "checkpoint_latest = ModelCheckpoint(\n",
    "    LATEST_MODEL_PATH, save_best_only=False, verbose=1\n",
    ")\n",
    "checkpoint_best = ModelCheckpoint(\n",
    "    BEST_MODEL_PATH, save_best_only=True,\n",
    "    monitor='val_accuracy', mode='max', verbose=1\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "csv_logger = CSVLogger(LOG_CSV, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTracker(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # save next starting epoch\n",
    "        with open(EPOCH_FILE, 'w') as f:\n",
    "            f.write(str(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "05QJdbXzhZc5"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    checkpoint_latest,\n",
    "    checkpoint_best,\n",
    "    early_stop,\n",
    "    csv_logger,\n",
    "    EpochTracker()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps_per_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ðŸ”Ÿ Launch Training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      3\u001b[0m     data_generator(TRAIN_H5, BATCH_SIZE),\n\u001b[0;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mdata_generator(VAL_H5, BATCH_SIZE),\n\u001b[1;32m----> 5\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[43msteps_per_epoch\u001b[49m,\n\u001b[0;32m      6\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mval_steps,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m      8\u001b[0m     initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[0;32m      9\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m     10\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'steps_per_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# ðŸ”Ÿ Launch Training\n",
    "history = model.fit(\n",
    "    data_generator(TRAIN_H5, BATCH_SIZE),\n",
    "    validation_data=data_generator(VAL_H5, BATCH_SIZE),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVfs0FIs3NnimPt+BO388",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
