{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Recognition Model Training\n",
    "- 1001 Classes\n",
    "- CPU Training\n",
    "- Real-time Inference Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "X_train = np.load('MS_train/processed_data/landmarks_train.npy', allow_pickle=True)\n",
    "y_train = np.load('MS_train/processed_data/labels_train.npy', allow_pickle=True)\n",
    "X_val = np.load('MS_train/processed_data/landmarks_val.npy', allow_pickle=True)\n",
    "y_val = np.load('MS_train/processed_data/labels_val.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "seq_lengths = [len(seq) for seq in X_train]\n",
    "print(f\"Sequence Length Analysis:\\n\"\n",
    "      f\"- Average: {np.mean(seq_lengths):.1f}\\n\"\n",
    "      f\"- Max: {np.max(seq_lengths)}\\n\"\n",
    "      f\"- Min: {np.min(seq_lengths)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seq_lengths, bins=50)\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.xlabel('Number of Frames')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution Analysis\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(f\"Class Distribution:\\n\"\n",
    "      f\"- Most common class: {class_counts.idxmax()} ({class_counts.max()} samples)\\n\"\n",
    "      f\"- Least common class: {class_counts.idxmin()} ({class_counts.min()} samples)\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(class_counts)), class_counts.values)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "MAX_SEQ_LENGTH = int(np.percentile(seq_lengths, 95))  # Covers 95% of samples\n",
    "print(f\"Using sequence length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# Padding sequences\n",
    "X_train_padded = pad_sequences(X_train, maxlen=MAX_SEQ_LENGTH, padding='post', dtype='float32')\n",
    "X_val_padded = pad_sequences(X_val, maxlen=MAX_SEQ_LENGTH, padding='post', dtype='float32')\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CPU-optimized model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0., input_shape=(MAX_SEQ_LENGTH, 126)),\n",
    "    LSTM(64, return_sequences=False, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1001, activation='softmax')\n",
    "])\n",
    "\n",
    "# Custom learning rate for better convergence\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "batch_size = 32  # Reduced for CPU memory\n",
    "epochs = 50\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model_cpu.h5', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_padded,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_val_padded, y_val_enc),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final assets\n",
    "model.save('sign_language_model.h5')\n",
    "np.save('label_encoder_classes.npy', le.classes_)\n",
    "\n",
    "# Convert to TF.js format\n",
    "%pip install tensorflowjs\n",
    "!tensorflowjs_converter --input_format keras sign_language_model.h5 tfjs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = [len(seq) for seq in X_train]\n",
    "val_lengths = [len(seq) for seq in X_val]\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(f\"- Average sequence length: {np.mean(train_lengths):.1f}\")\n",
    "print(f\"- 95th percentile: {np.percentile(train_lengths, 95)}\")\n",
    "print(f\"- Max length: {np.max(train_lengths)}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"- Average sequence length: {np.mean(val_lengths):.1f}\")\n",
    "print(f\"- 95th percentile: {np.percentile(val_lengths, 95)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance Analysis\n",
    "class_distribution = pd.Series(y_train).value_counts().sort_values(ascending=False)\n",
    "print(f\"Class Balance Metrics:\")\n",
    "print(f\"- Class count range: {class_distribution.min()} to {class_distribution.max()}\")\n",
    "print(f\"- Median samples per class: {class_distribution.median()}\")\n",
    "print(f\"- Classes with <10 samples: {(class_distribution < 10).sum()}\")\n",
    "\n",
    "# Long-tail visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(class_distribution.values)\n",
    "plt.title('Class Distribution (Sorted by Frequency)')\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distribution Analysis\n",
    "all_landmarks = np.concatenate(X_train_padded)\n",
    "print(\"Landmark Value Distribution:\")\n",
    "print(f\"- X range: [{np.min(all_landmarks):.3f}, {np.max(all_landmarks):.3f}]\")\n",
    "print(f\"- Mean: {np.mean(all_landmarks):.3f}\")\n",
    "print(f\"- Std Dev: {np.std(all_landmarks):.3f}\")\n",
    "\n",
    "# Plot coordinate distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_landmarks.flatten(), bins=100, log=True)\n",
    "plt.title('Landmark Coordinate Distribution')\n",
    "plt.xlabel('Normalized Coordinate Value')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Predictions\n",
    "test_samples = 5\n",
    "sample_indices = np.random.choice(len(X_val), test_samples)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    prediction = model.predict(X_val_padded[idx][np.newaxis, ...])\n",
    "    true_label = le.inverse_transform([y_val_enc[idx]])[0]\n",
    "    pred_label = le.inverse_transform([np.argmax(prediction)])[0]\n",
    "    \n",
    "    plt.subplot(test_samples, 1, i+1)\n",
    "    plt.bar(range(1001), prediction[0])\n",
    "    plt.title(f'True: {true_label} | Pred: {pred_label}')\n",
    "    plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (Top 50 Classes)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "top_classes = class_distribution.index[:50]\n",
    "mask = np.isin(y_val_enc, le.transform(top_classes))\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    y_val_enc[mask],\n",
    "    np.argmax(model.predict(X_val_padded[mask]), axis=1),\n",
    "    labels=le.transform(top_classes)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=top_classes,\n",
    "            yticklabels=top_classes)\n",
    "plt.title('Top 50 Classes Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time Inference Simulation\n",
    "def real_time_demo():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hands = mp.solutions.hands.Hands()\n",
    "    \n",
    "    sequence = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            left, right = extract_landmarks_from_frame(frame, hands)\n",
    "            sequence.extend(left)\n",
    "            sequence.extend(right)\n",
    "            \n",
    "            if len(sequence) >= MAX_SEQ_LENGTH * 126:\n",
    "                input_data = pad_sequences([sequence[-MAX_SEQ_LENGTH*126:]], \n",
    "                                        maxlen=MAX_SEQ_LENGTH*126,\n",
    "                                        padding='post',\n",
    "                                        dtype='float32')\n",
    "                prediction = model.predict(input_data.reshape(1, MAX_SEQ_LENGTH, 126))\n",
    "                pred_class = le.inverse_transform([np.argmax(prediction)])[0]\n",
    "                cv2.putText(frame, pred_class, (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "        cv2.imshow('Sign Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Uncomment to run demo\n",
    "# real_time_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
